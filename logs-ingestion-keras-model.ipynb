{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63deaf17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836589f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Helpers\n",
    "\n",
    "def categorize_status_code(status_code):\n",
    "    if status_code >= 500 and status_code <= 600:\n",
    "        return 0\n",
    "    elif status_code >= 200 and status_code <= 300:\n",
    "        return 1\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e9e7c7",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7c3c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xtimestamp': datetime.datetime(2023, 6, 6, 7, 35, 12), 'agg_timestamp': '2023:06:06 07:35', 'day_of_week': 'Tuesday', 'hour': '07', 'server': 'interswitch-virtual-machine', 'client_ip': '41.210.186.148', 'client_port': 51732, 'frontend': 'www-https', 'backend': 'interswitchstore_backend', 'backendservername': 'interswitchstore_webserver', 'timing': '371/0/1/37/409', 'http_status': 1, 'bytes_read': 217, 'request_cookie': '-', 'response_cookie': '-', 'termination_state': '-', 'act_conn': 452, 'fe_conn': 452, 'be_conn': 0, 'srv_conn': 1, 'act_time': 371, 'fe_time': 0, 'be_time': 1, 'srv_time': 37, 'total_time': 409, 'act_sess': 0, 'fe_sess': 0, 'be_sess': 0, 'srv_sess': 0, 'retries': 0, 'http_method': 'POST', 'endpoint': '/interswitchstore/device/remoteConfigs', 'http_version': 'HTTP/1.1', 'count_value': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log_entries = []\n",
    "\n",
    "# Read the log file\n",
    "with open('data/haproxy.log', 'r') as file:\n",
    "    log_entries = file.readlines()\n",
    "\n",
    "# Define regular expression for field extraction\n",
    "regex_pattern = r'^(?P<timestamp>\\w+\\s+\\d+\\s\\d+:\\d+:\\d+)\\s+(?P<server>\\S+)\\s+haproxy\\[\\d+\\]:\\s(?P<client_ip>[\\d.:]+):(?P<client_port>\\d+)\\s+\\[(?P<datetime>[^\\]]+)\\]\\s+(?P<frontend>\\S+)~\\s+(?P<backend>[^\\s/]+)/(?P<backendservername>[^\\s]+)\\s+(?P<timing>[^\\s]+)\\s+(?P<http_status>\\d+)\\s+(?P<bytes_read>\\d+)\\s+.*\\s+(?P<act_conn>\\d+)/(?P<fe_conn>\\d+)/(?P<be_conn>\\d+)/(?P<srv_conn>\\d+)/(?P<retries>\\d+)\\s+.*\\s+\"(?P<request_line>[^\"]+)\"'\n",
    "\n",
    "fields = []\n",
    "\n",
    "# Iterate through log entries\n",
    "for log_entry in log_entries:\n",
    "    # Extract fields using regular expression\n",
    "    match = re.match(regex_pattern, log_entry)\n",
    "    if match:\n",
    "        \n",
    "         # Convert timestamp to datetime value\n",
    "        timestamp_str = match.group('timestamp')\n",
    "        timestamp = datetime.strptime(timestamp_str, '%b %d %H:%M:%S')\n",
    "        # Set the current year as the placeholder\n",
    "        current_year = datetime.now().year\n",
    "        timestamp = timestamp.replace(year=current_year)\n",
    "        \n",
    "        \n",
    "         # Convert datetime to datetime value\n",
    "        datetime_str =  match.group('datetime')\n",
    "        datetime_format = '%d/%b/%Y:%H:%M:%S.%f'\n",
    "        mdatetime = datetime.strptime(datetime_str, datetime_format).timestamp()\n",
    "        \n",
    "        timing_dict=''\n",
    "        \n",
    "        timing_columns = ['act_time', 'fe_time', 'be_time', 'srv_time', 'total_time']\n",
    "        values_list = match.group('timing').split(\"/\")\n",
    "        timing_dict = {column: int(value) for column, value in zip(timing_columns, values_list)}\n",
    "        \n",
    "        # Split the request line by spaces\n",
    "        request_line_parts = match.group('request_line').split()\n",
    "        http_method = request_line_parts[0]\n",
    "        endpoint = request_line_parts[1]\n",
    "        http_version = request_line_parts[2]\n",
    "\n",
    "        #print(timing_dict)\n",
    "        \n",
    "        \n",
    "        if \"/svapayments/transactions/\" in endpoint:\n",
    "            new_endpoint = endpoint.split(\"/svapayments/transactions/\")[0] + \"/svapayments/transactions/\"\n",
    "        elif \"/api/v2/quickteller/agent/accountBalance/\" in endpoint:\n",
    "            new_endpoint = endpoint.split(\"/api/v2/quickteller/agent/accountBalance/\")[0] + \"/api/v2/quickteller/agent/accountBalance/\"\n",
    "        elif \"/api/v1/appservice/users/\" in endpoint:\n",
    "            new_endpoint = endpoint.split(\"/api/v1/appservice/users/\")[0] + \"/api/v1/appservice/users/\"\n",
    "        elif \"/tugende/recon/v1/check-transaction-status/\" in endpoint:\n",
    "            new_endpoint = endpoint.split(\"/tugende/recon/v1/check-transaction-status/\")[0] + \"/tugende/recon/v1/check-transaction-status/\"\n",
    "        elif \"/api/v2/quickteller/agent/appNotifications/\" in endpoint:\n",
    "            new_endpoint = endpoint.split(\"/api/v2/quickteller/agent/appNotifications/\")[0] + \"/api/v2/quickteller/agent/appNotifications/\"\n",
    "        \n",
    "        else:\n",
    "            new_endpoint = endpoint\n",
    "        \n",
    "        fields.append({\n",
    "            'xtimestamp': timestamp,\n",
    "            'agg_timestamp': timestamp.strftime('%Y:%m:%d %H:%M'),\n",
    "            'day_of_week': timestamp.strftime('%A'),\n",
    "            'hour': timestamp.strftime('%H'),\n",
    "            'server': match.group('server'),\n",
    "            'client_ip': match.group('client_ip'),\n",
    "            'client_port': int(match.group('client_port')),\n",
    "            #'xdatetime': mdatetime,\n",
    "            'frontend': match.group('frontend'),\n",
    "            'backend': match.group('backend'),\n",
    "            'backendservername': match.group('backendservername'),\n",
    "            'timing': match.group('timing'),\n",
    "            'http_status': categorize_status_code(int(match.group('http_status'))),\n",
    "            'bytes_read': int(match.group('bytes_read')),\n",
    "            'request_cookie': '-',\n",
    "            'response_cookie': '-',\n",
    "            'termination_state': '-',\n",
    "            'act_conn': int(match.group('act_conn')),\n",
    "            'fe_conn': int(match.group('fe_conn')),\n",
    "            'be_conn': int(match.group('be_conn')),\n",
    "            'srv_conn': int(match.group('srv_conn')),\n",
    "            'act_time': int(timing_dict['act_time']),\n",
    "            'fe_time': int(timing_dict['fe_time']),\n",
    "            'be_time': int(timing_dict['be_time']),\n",
    "            'srv_time': int(timing_dict['srv_time']),\n",
    "            'total_time': int(timing_dict['total_time']),\n",
    "            'act_sess': 0,\n",
    "            'fe_sess': 0,\n",
    "            'be_sess': 0,\n",
    "            'srv_sess': 0,\n",
    "            'retries': int(match.group('retries')),\n",
    "            #'request_line': match.group('request_line'),\n",
    "            'http_method': http_method,\n",
    "            'endpoint': new_endpoint,\n",
    "            'http_version': http_version,\n",
    "            'count_value': 1\n",
    "            \n",
    "        })\n",
    "\n",
    "# Print the first n extracted fields\n",
    "for field in fields[:1]:\n",
    "    print(field)\n",
    "    \n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01283257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.isnull().sum())\n",
    "## Drop null\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a1c45",
   "metadata": {},
   "source": [
    "## Perform analysis on the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e703df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Counts:\n",
      "/api/v2/quickteller/agent/accountBalance/                                                                                        695483\n",
      "/api/v1A/svapayments/validateCustomer                                                                                            572551\n",
      "/interswitchstore/device/trackupdate                                                                                             140157\n",
      "/interswitchstore/device/geolocationstatus                                                                                       130513\n",
      "/api/v1A/svapayments/sendAdviceRequest                                                                                            92300\n",
      "                                                                                                                                  ...  \n",
      "/extraswitch/reportDownload.do?reportId=272680078&show=attachment&con=EBO_SACCO_Issuer_pdf_2023_0606_012008.pdf.pdf                   1\n",
      "/extraswitch/reportDownload.do?reportId=272680082&show=attachment&con=KCB_Issuer%20Detail%20Report_2023_0606_012004.pdf.pdf           1\n",
      "/extraswitch/login.do;jsessionid=BE0FA7539DB0C0D8DBE4F1A76DB2B4F6                                                                     1\n",
      "/extraswitch/reportDownload.do?reportId=272679933&show=attachment&con=KCB_Acquired%20Detail%20Report_2023_0606_012005.pdf.pdf         1\n",
      "/api/v2/quickteller/agent/transactionsSummary/3is19302                                                                                1\n",
      "Name: endpoint, Length: 14004, dtype: int64\n",
      "\n",
      "Response Counts:\n",
      "1.0    2021100\n",
      "0.0     349264\n",
      "Name: http_status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Request Analysis\n",
    "request_counts = df['endpoint'].value_counts()\n",
    "print(\"Request Counts:\")\n",
    "print(request_counts)\n",
    "\n",
    "\n",
    "# Response Analysis\n",
    "response_counts = df['http_status'].value_counts()\n",
    "print(\"\\nResponse Counts:\")\n",
    "print(response_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c786fd82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timing Summary:\n",
      "           act_time       fe_time       be_time      srv_time    total_time\n",
      "count  2.370364e+06  2.370364e+06  2.370364e+06  2.370364e+06  2.370364e+06\n",
      "mean   6.305557e+03 -5.943349e-02  5.888496e-01  1.600037e+03  9.825243e+03\n",
      "std    2.200778e+04  2.364343e-01  3.676219e+01  9.693202e+03  3.230876e+04\n",
      "min   -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00  4.000000e+00\n",
      "25%    6.600000e+01  0.000000e+00  0.000000e+00  3.000000e+01  1.700000e+02\n",
      "50%    2.070000e+02  0.000000e+00  0.000000e+00  9.400000e+01  5.340000e+02\n",
      "75%    1.403000e+03  0.000000e+00  1.000000e+00  5.080000e+02  3.163000e+03\n",
      "max    2.624990e+05  0.000000e+00  1.501000e+04  2.557710e+05  4.796950e+05\n"
     ]
    }
   ],
   "source": [
    "# Timing Analysis\n",
    "timing_columns = ['act_time', 'fe_time', 'be_time', 'srv_time', 'total_time']\n",
    "df[timing_columns] = df['timing'].str.split('/', expand=True).astype(int)\n",
    "timing_summary = df[timing_columns].describe()\n",
    "print(\"\\nTiming Summary:\")\n",
    "print(timing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba8b1fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connection Summary:\n",
      "           act_conn       fe_conn       be_conn      srv_conn\n",
      "count  2.370364e+06  2.370364e+06  2.370364e+06  2.370364e+06\n",
      "mean   3.013297e+03  3.013277e+03  5.213312e+01  1.911712e+01\n",
      "std    8.170410e+02  8.170341e+02  1.766426e+02  8.419456e+01\n",
      "min    4.050000e+02  4.050000e+02  0.000000e+00  0.000000e+00\n",
      "25%    2.545000e+03  2.545000e+03  2.000000e+00  2.000000e+00\n",
      "50%    3.332000e+03  3.332000e+03  7.000000e+00  4.000000e+00\n",
      "75%    3.516000e+03  3.516000e+03  3.400000e+01  1.200000e+01\n",
      "max    5.219000e+03  5.219000e+03  2.125000e+03  2.125000e+03\n"
     ]
    }
   ],
   "source": [
    "# Connection Analysis\n",
    "connection_columns = ['act_conn', 'fe_conn', 'be_conn', 'srv_conn']\n",
    "connection_summary = df[connection_columns].describe()\n",
    "print(\"\\nConnection Summary:\")\n",
    "print(connection_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98af6ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bytes Summary:\n",
      "count    2.370364e+06\n",
      "mean     3.676995e+03\n",
      "std      1.074225e+05\n",
      "min      1.060000e+02\n",
      "25%      3.140000e+02\n",
      "50%      5.140000e+02\n",
      "75%      5.180000e+02\n",
      "max      6.900052e+07\n",
      "Name: bytes_read, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Bytes Analysis\n",
    "bytes_summary = df['bytes_read'].describe()\n",
    "print(\"\\nBytes Summary:\")\n",
    "print(bytes_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b026903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# # Initialize the FeatureHasher\n",
    "# hasher = FeatureHasher(n_features=1, input_type='string')\n",
    "\n",
    "# # Apply the Hashing Trick on the 'category' column\n",
    "# hashed_features = hasher.transform(df['day_of_week'])\n",
    "\n",
    "# # Convert the hashed features to a NumPy array\n",
    "# hashed_array = hashed_features.toarray()\n",
    "\n",
    "# # Create a new DataFrame with the hashed features\n",
    "# hashed_df = pd.DataFrame(hashed_array)\n",
    "\n",
    "# # Concatenate the original DataFrame with the hashed DataFrame\n",
    "# df_hashed = pd.concat([df, hashed_df], axis=1)\n",
    "\n",
    "# # Print the resulting DataFrame\n",
    "# print(df_hashed.shape)\n",
    "# df_hashed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90149496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# yearly_traffic.plot(kind='bar', color='blue')\n",
    "# plt.title('Yearly Traffic')\n",
    "# plt.xlabel('Year')\n",
    "# plt.ylabel('Bytes Read')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd456e",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "1. Date and Time Features: Extract additional information from the timestamp field, such as hour of the day, day of the week, month, etc. These features can help capture any temporal patterns or trends in the data.\n",
    "\n",
    "2. Categorical Encoding: Convert categorical variables like server, frontend, backend, etc., into numeric representations using one-hot encoding or label encoding. This allows machine learning models to work with categorical data effectively.\n",
    "\n",
    "3. Session Duration: Calculate the duration of each session by subtracting the start and end timestamps. This can provide insights into session lengths and help identify sessions with unusually long or short durations.\n",
    "\n",
    "4. Request Analysis: Extract features from the request_line, such as the HTTP method (GET, POST, etc.), endpoint, or API version. These features can help analyze request patterns and identify popular endpoints or API versions.\n",
    "\n",
    "5. Response Analysis: Create binary features based on the HTTP status code, such as whether the response was successful (200-299) or had an error (400-599). This can help identify patterns in successful and failed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a91ed34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2370364, 34)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xtimestamp</th>\n",
       "      <th>agg_timestamp</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>server</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>client_port</th>\n",
       "      <th>frontend</th>\n",
       "      <th>backend</th>\n",
       "      <th>backendservername</th>\n",
       "      <th>...</th>\n",
       "      <th>total_time</th>\n",
       "      <th>act_sess</th>\n",
       "      <th>fe_sess</th>\n",
       "      <th>be_sess</th>\n",
       "      <th>srv_sess</th>\n",
       "      <th>retries</th>\n",
       "      <th>http_method</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>http_version</th>\n",
       "      <th>count_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-06 07:35:12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11490</td>\n",
       "      <td>51732</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11114</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-06 07:35:12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6347</td>\n",
       "      <td>59225</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1918</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-06 07:35:12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6481</td>\n",
       "      <td>1517</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11113</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-06 07:35:12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6347</td>\n",
       "      <td>59225</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1918</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-06 07:35:13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6347</td>\n",
       "      <td>59225</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1918</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           xtimestamp  agg_timestamp  day_of_week  hour  server  client_ip  \\\n",
       "0 2023-06-06 07:35:12              0            0     0       0      11490   \n",
       "1 2023-06-06 07:35:12              0            0     0       0       6347   \n",
       "2 2023-06-06 07:35:12              0            0     0       0       6481   \n",
       "3 2023-06-06 07:35:12              0            0     0       0       6347   \n",
       "4 2023-06-06 07:35:13              0            0     0       0       6347   \n",
       "\n",
       "   client_port  frontend  backend  backendservername  ...  total_time  \\\n",
       "0        51732         0        6                  9  ...         409   \n",
       "1        59225         0        1                  3  ...         191   \n",
       "2         1517         0        6                  9  ...         173   \n",
       "3        59225         0        1                  2  ...         246   \n",
       "4        59225         0        1                  3  ...         237   \n",
       "\n",
       "   act_sess  fe_sess  be_sess  srv_sess  retries  http_method  endpoint  \\\n",
       "0         0        0        0         0        0            3     11114   \n",
       "1         0        0        0         0        0            3      1918   \n",
       "2         0        0        0         0        0            3     11113   \n",
       "3         0        0        0         0        0            3      1918   \n",
       "4         0        0        0         0        0            3      1918   \n",
       "\n",
       "   http_version  count_value  \n",
       "0             1            1  \n",
       "1             1            1  \n",
       "2             1            1  \n",
       "3             1            1  \n",
       "4             1            1  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Iterate over each column in the DataFrame\n",
    "for column in df.columns:\n",
    "    # Check if the column contains categorical data\n",
    "    if df[column].dtype == 'object':\n",
    "        # Perform integer encoding on the column\n",
    "        df[column] = label_encoder.fit_transform(df[column].astype(str))\n",
    "        \n",
    "        \n",
    "# # Print the resulting DataFrame\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc7e824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical values\n",
    "# Select only the categorical columns\n",
    "# categorical_columns = df.select_dtypes(include='object').columns\n",
    "\n",
    "# Apply one-hot encoding to categorical columns\n",
    "# df_encoded = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "# Print the encoded DataFrame\n",
    "# print(df_encoded.shape)\n",
    "# df_encoded.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864d7f48",
   "metadata": {},
   "source": [
    "## Model Selection: \n",
    "Explore various deep learning models that are suitable for time-series prediction tasks. Some popular models include Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRU), Convolutional Neural Networks (CNN), or Transformer-based architectures. These models can capture temporal dependencies and patterns in the log data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0532cc",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16b2a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Preprocess the data and prepare X and y\n",
    "# df_filtered = df[df['http_status'] != 'other']\n",
    "# X = df_filtered.drop(['http_status'], axis=1)\n",
    "# y = df_filtered['http_status']\n",
    "\n",
    "# # Encode the categorical target variable\n",
    "# encoder = OneHotEncoder(sparse=False)\n",
    "# y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# # Split the data into training and testing datasets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Build the TensorFlow model\n",
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Dense(64, activation='relu', input_shape=(2,1)),\n",
    "#     tf.keras.layers.Dense(64, activation='relu'),\n",
    "#     tf.keras.layers.Dense(2, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d56e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "348250/948157 [==========>...................] - ETA: 8:18 - loss: 9.1400 - accuracy: 0.8516"
     ]
    }
   ],
   "source": [
    "#Build the neural network model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 5\n",
    "INIT_LEARNING_RATE = 0.001\n",
    "dense_units = 50\n",
    "activation_func = 'relu'\n",
    "activation_func_out='sigmoid'\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "# Separate the independent and dependent variables\n",
    "X = df.drop(['http_status', 'xtimestamp'], axis=1)\n",
    "y_status = df['http_status']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y_status, test_size=0.1, random_state=42)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1111, random_state=42)\n",
    "\n",
    "# Define the model architecture\n",
    "# Define the input layer\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# Add multiple hidden layers\n",
    "dense1 = Dense(64, activation='relu')(inputs)\n",
    "dropout1 = Dropout(0.2)(dense1)\n",
    "dense2 = Dense(32, activation='relu')(dropout1)\n",
    "dropout2 = Dropout(0.2)(dense2)\n",
    "dense3 = Dense(16, activation='relu')(dropout2)\n",
    "\n",
    "# Add the output layer\n",
    "output = Dense(1, activation='sigmoid')(dense3)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=INIT_LEARNING_RATE), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905ec046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2376657, 34)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xtimestamp</th>\n",
       "      <th>agg_timestamp</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>server</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>client_port</th>\n",
       "      <th>frontend</th>\n",
       "      <th>backend</th>\n",
       "      <th>backendservername</th>\n",
       "      <th>...</th>\n",
       "      <th>total_time</th>\n",
       "      <th>act_sess</th>\n",
       "      <th>fe_sess</th>\n",
       "      <th>be_sess</th>\n",
       "      <th>srv_sess</th>\n",
       "      <th>retries</th>\n",
       "      <th>http_method</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>http_version</th>\n",
       "      <th>count_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-06 07:35:12</td>\n",
       "      <td>2023:06:06 07:35</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>07</td>\n",
       "      <td>interswitch-virtual-machine</td>\n",
       "      <td>41.210.186.148</td>\n",
       "      <td>51732</td>\n",
       "      <td>www-https</td>\n",
       "      <td>interswitchstore_backend</td>\n",
       "      <td>interswitchstore_webserver</td>\n",
       "      <td>...</td>\n",
       "      <td>409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POST</td>\n",
       "      <td>/interswitchstore/device/remoteConfigs</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-06 07:35:12</td>\n",
       "      <td>2023:06:06 07:35</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>07</td>\n",
       "      <td>interswitch-virtual-machine</td>\n",
       "      <td>196.13.203.201</td>\n",
       "      <td>59225</td>\n",
       "      <td>www-https</td>\n",
       "      <td>api_backend</td>\n",
       "      <td>api_webserver2</td>\n",
       "      <td>...</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POST</td>\n",
       "      <td>/api/v1A/svapayments/validateCustomer</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-06 07:35:12</td>\n",
       "      <td>2023:06:06 07:35</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>07</td>\n",
       "      <td>interswitch-virtual-machine</td>\n",
       "      <td>197.239.10.226</td>\n",
       "      <td>1517</td>\n",
       "      <td>www-https</td>\n",
       "      <td>interswitchstore_backend</td>\n",
       "      <td>interswitchstore_webserver</td>\n",
       "      <td>...</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POST</td>\n",
       "      <td>/interswitchstore/device/geolocationstatus</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-06 07:35:12</td>\n",
       "      <td>2023:06:06 07:35</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>07</td>\n",
       "      <td>interswitch-virtual-machine</td>\n",
       "      <td>196.13.203.201</td>\n",
       "      <td>59225</td>\n",
       "      <td>www-https</td>\n",
       "      <td>api_backend</td>\n",
       "      <td>api_webserver1</td>\n",
       "      <td>...</td>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POST</td>\n",
       "      <td>/api/v1A/svapayments/validateCustomer</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-06 07:35:13</td>\n",
       "      <td>2023:06:06 07:35</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>07</td>\n",
       "      <td>interswitch-virtual-machine</td>\n",
       "      <td>196.13.203.201</td>\n",
       "      <td>59225</td>\n",
       "      <td>www-https</td>\n",
       "      <td>api_backend</td>\n",
       "      <td>api_webserver2</td>\n",
       "      <td>...</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POST</td>\n",
       "      <td>/api/v1A/svapayments/validateCustomer</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           xtimestamp     agg_timestamp day_of_week hour  \\\n",
       "0 2023-06-06 07:35:12  2023:06:06 07:35     Tuesday   07   \n",
       "1 2023-06-06 07:35:12  2023:06:06 07:35     Tuesday   07   \n",
       "2 2023-06-06 07:35:12  2023:06:06 07:35     Tuesday   07   \n",
       "3 2023-06-06 07:35:12  2023:06:06 07:35     Tuesday   07   \n",
       "4 2023-06-06 07:35:13  2023:06:06 07:35     Tuesday   07   \n",
       "\n",
       "                        server       client_ip  client_port   frontend  \\\n",
       "0  interswitch-virtual-machine  41.210.186.148        51732  www-https   \n",
       "1  interswitch-virtual-machine  196.13.203.201        59225  www-https   \n",
       "2  interswitch-virtual-machine  197.239.10.226         1517  www-https   \n",
       "3  interswitch-virtual-machine  196.13.203.201        59225  www-https   \n",
       "4  interswitch-virtual-machine  196.13.203.201        59225  www-https   \n",
       "\n",
       "                    backend           backendservername  ... total_time  \\\n",
       "0  interswitchstore_backend  interswitchstore_webserver  ...        409   \n",
       "1               api_backend              api_webserver2  ...        191   \n",
       "2  interswitchstore_backend  interswitchstore_webserver  ...        173   \n",
       "3               api_backend              api_webserver1  ...        246   \n",
       "4               api_backend              api_webserver2  ...        237   \n",
       "\n",
       "   act_sess  fe_sess be_sess srv_sess retries  http_method  \\\n",
       "0         0        0       0        0       0         POST   \n",
       "1         0        0       0        0       0         POST   \n",
       "2         0        0       0        0       0         POST   \n",
       "3         0        0       0        0       0         POST   \n",
       "4         0        0       0        0       0         POST   \n",
       "\n",
       "                                     endpoint  http_version  count_value  \n",
       "0      /interswitchstore/device/remoteConfigs      HTTP/1.1            1  \n",
       "1       /api/v1A/svapayments/validateCustomer      HTTP/1.1            1  \n",
       "2  /interswitchstore/device/geolocationstatus      HTTP/1.1            1  \n",
       "3       /api/v1A/svapayments/validateCustomer      HTTP/1.1            1  \n",
       "4       /api/v1A/svapayments/validateCustomer      HTTP/1.1            1  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(df.drop('http_status', axis=1).values)\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d894836",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc289c2",
   "metadata": {},
   "source": [
    "## Training and Evaluation:\n",
    "Split your dataset into training, validation, and testing sets. Train your deep learning model on the training data and fine-tune hyperparameters using the validation set. Evaluate the model's performance using appropriate metrics like accuracy, precision, recall, F1-score, or area under the ROC curve (AUC-ROC)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb7c28",
   "metadata": {},
   "source": [
    "## Model Interpretability:\n",
    "Consider using techniques that enhance model interpretability, especially in critical systems like predicting downtime. Methods like attention mechanisms, feature importance analysis, or SHAP (SHapley Additive exPlanations) values can help you understand the factors contributing to downtime predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3498a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
