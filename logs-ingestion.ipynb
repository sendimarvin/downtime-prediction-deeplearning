{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "63deaf17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "836589f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Helpers\n",
    "\n",
    "def categorize_status_code(status_code):\n",
    "    if status_code >= 500 and status_code <= 600:\n",
    "        return \"downtime\"\n",
    "    elif status_code >= 200 and status_code <= 300:\n",
    "        return \"uptime\"\n",
    "    else:\n",
    "        return \"other\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e9e7c7",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2a7c3c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xtimestamp': datetime.datetime(2023, 6, 6, 12, 49, 8), 'agg_timestamp': '2023:06:06 12:49', 'day_of_week': 'Tuesday', 'hour': '12', 'server': 'interswitch-virtual-machine', 'client_ip': '41.75.170.169', 'client_port': 13805, 'frontend': 'www-https', 'backend': 'svaAgentLogin_backend', 'backendservername': 'api_webserver5', 'timing': '19203/0/0/525/19735', 'http_status': 'uptime', 'bytes_read': 517, 'request_cookie': '-', 'response_cookie': '-', 'termination_state': '-', 'act_conn': 4046, 'fe_conn': 4046, 'be_conn': 101, 'srv_conn': 54, 'act_time': 19203, 'fe_time': 0, 'be_time': 0, 'srv_time': 525, 'total_time': 19735, 'act_sess': 0, 'fe_sess': 0, 'be_sess': 0, 'srv_sess': 0, 'retries': 0, 'http_method': 'GET', 'endpoint': '/api/v2/quickteller/agent/accountBalance/23027333', 'http_version': 'HTTP/1.1', 'count_value': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log_entries = []\n",
    "\n",
    "# Read the log file\n",
    "with open('data/haproxytmp.log', 'r') as file:\n",
    "    log_entries = file.readlines()\n",
    "\n",
    "# Define regular expression for field extraction\n",
    "regex_pattern = r'^(?P<timestamp>\\w+\\s+\\d+\\s\\d+:\\d+:\\d+)\\s+(?P<server>\\S+)\\s+haproxy\\[\\d+\\]:\\s(?P<client_ip>[\\d.:]+):(?P<client_port>\\d+)\\s+\\[(?P<datetime>[^\\]]+)\\]\\s+(?P<frontend>\\S+)~\\s+(?P<backend>[^\\s/]+)/(?P<backendservername>[^\\s]+)\\s+(?P<timing>[^\\s]+)\\s+(?P<http_status>\\d+)\\s+(?P<bytes_read>\\d+)\\s+.*\\s+(?P<act_conn>\\d+)/(?P<fe_conn>\\d+)/(?P<be_conn>\\d+)/(?P<srv_conn>\\d+)/(?P<retries>\\d+)\\s+.*\\s+\"(?P<request_line>[^\"]+)\"'\n",
    "\n",
    "fields = []\n",
    "\n",
    "# Iterate through log entries\n",
    "for log_entry in log_entries:\n",
    "    # Extract fields using regular expression\n",
    "    match = re.match(regex_pattern, log_entry)\n",
    "    if match:\n",
    "        \n",
    "         # Convert timestamp to datetime value\n",
    "        timestamp_str = match.group('timestamp')\n",
    "        timestamp = datetime.strptime(timestamp_str, '%b %d %H:%M:%S')\n",
    "        # Set the current year as the placeholder\n",
    "        current_year = datetime.now().year\n",
    "        timestamp = timestamp.replace(year=current_year)\n",
    "        \n",
    "        \n",
    "         # Convert datetime to datetime value\n",
    "        datetime_str =  match.group('datetime')\n",
    "        datetime_format = '%d/%b/%Y:%H:%M:%S.%f'\n",
    "        mdatetime = datetime.strptime(datetime_str, datetime_format).timestamp()\n",
    "        \n",
    "        timing_dict['act_time']=0\n",
    "        timing_dict['fe_time']=0\n",
    "        timing_dict['be_time']=0\n",
    "        timing_dict['srv_time']=0\n",
    "        timing_dict['total_time']=0\n",
    "        \n",
    "        timing_columns = ['act_time', 'fe_time', 'be_time', 'srv_time', 'total_time']\n",
    "        values_list = match.group('timing').split(\"/\")\n",
    "        timing_dict = {column: int(value) for column, value in zip(timing_columns, values_list)}\n",
    "        \n",
    "        # Split the request line by spaces\n",
    "        request_line_parts = match.group('request_line').split()\n",
    "        http_method = request_line_parts[0]\n",
    "        endpoint = request_line_parts[1]\n",
    "        http_version = request_line_parts[2]\n",
    "\n",
    "        #print(timing_dict)\n",
    "        \n",
    "        fields.append({\n",
    "            'xtimestamp': timestamp,\n",
    "            'agg_timestamp': timestamp.strftime('%Y:%m:%d %H:%M'),\n",
    "            'day_of_week': timestamp.strftime('%A'),\n",
    "            'hour': timestamp.strftime('%H'),\n",
    "            'server': match.group('server'),\n",
    "            'client_ip': match.group('client_ip'),\n",
    "            'client_port': int(match.group('client_port')),\n",
    "            #'xdatetime': mdatetime,\n",
    "            'frontend': match.group('frontend'),\n",
    "            'backend': match.group('backend'),\n",
    "            'backendservername': match.group('backendservername'),\n",
    "            'timing': match.group('timing'),\n",
    "            'http_status': categorize_status_code(int(match.group('http_status'))),\n",
    "            'bytes_read': int(match.group('bytes_read')),\n",
    "            'request_cookie': '-',\n",
    "            'response_cookie': '-',\n",
    "            'termination_state': '-',\n",
    "            'act_conn': int(match.group('act_conn')),\n",
    "            'fe_conn': int(match.group('fe_conn')),\n",
    "            'be_conn': int(match.group('be_conn')),\n",
    "            'srv_conn': int(match.group('srv_conn')),\n",
    "            'act_time': int(timing_dict['act_time']),\n",
    "            'fe_time': int(timing_dict['fe_time']),\n",
    "            'be_time': int(timing_dict['be_time']),\n",
    "            'srv_time': int(timing_dict['srv_time']),\n",
    "            'total_time': int(timing_dict['total_time']),\n",
    "            'act_sess': 0,\n",
    "            'fe_sess': 0,\n",
    "            'be_sess': 0,\n",
    "            'srv_sess': 0,\n",
    "            'retries': int(match.group('retries')),\n",
    "            #'request_line': match.group('request_line'),\n",
    "            'http_method': http_method,\n",
    "            'endpoint': endpoint,\n",
    "            'http_version': http_version,\n",
    "            'count_value': 1\n",
    "            \n",
    "        })\n",
    "\n",
    "# Print the first n extracted fields\n",
    "for field in fields[:1]:\n",
    "    print(field)\n",
    "    \n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "905ec046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3336, 34)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xtimestamp</th>\n",
       "      <th>agg_timestamp</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>server</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>client_port</th>\n",
       "      <th>frontend</th>\n",
       "      <th>backend</th>\n",
       "      <th>backendservername</th>\n",
       "      <th>...</th>\n",
       "      <th>total_time</th>\n",
       "      <th>act_sess</th>\n",
       "      <th>fe_sess</th>\n",
       "      <th>be_sess</th>\n",
       "      <th>srv_sess</th>\n",
       "      <th>retries</th>\n",
       "      <th>http_method</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>http_version</th>\n",
       "      <th>count_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-06 12:49:08</td>\n",
       "      <td>2023:06:06 12:49</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>12</td>\n",
       "      <td>interswitch-virtual-machine</td>\n",
       "      <td>41.75.170.169</td>\n",
       "      <td>13805</td>\n",
       "      <td>www-https</td>\n",
       "      <td>svaAgentLogin_backend</td>\n",
       "      <td>api_webserver5</td>\n",
       "      <td>...</td>\n",
       "      <td>19735</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GET</td>\n",
       "      <td>/api/v2/quickteller/agent/accountBalance/23027333</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-06 12:49:08</td>\n",
       "      <td>2023:06:06 12:49</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>12</td>\n",
       "      <td>interswitch-virtual-machine</td>\n",
       "      <td>41.75.170.75</td>\n",
       "      <td>15702</td>\n",
       "      <td>www-https</td>\n",
       "      <td>interswitchstore_backend</td>\n",
       "      <td>interswitchstore_webserver</td>\n",
       "      <td>...</td>\n",
       "      <td>194</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POST</td>\n",
       "      <td>/interswitchstore/device/geolocationstatus</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-06 12:49:08</td>\n",
       "      <td>2023:06:06 12:49</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>12</td>\n",
       "      <td>interswitch-virtual-machine</td>\n",
       "      <td>41.210.155.99</td>\n",
       "      <td>38338</td>\n",
       "      <td>www-https</td>\n",
       "      <td>interswitchstore_backend</td>\n",
       "      <td>interswitchstore_webserver</td>\n",
       "      <td>...</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POST</td>\n",
       "      <td>/interswitchstore/device/geolocationstatus</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-06 12:49:08</td>\n",
       "      <td>2023:06:06 12:49</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>12</td>\n",
       "      <td>interswitch-virtual-machine</td>\n",
       "      <td>197.221.137.205</td>\n",
       "      <td>34708</td>\n",
       "      <td>www-https</td>\n",
       "      <td>api_backend</td>\n",
       "      <td>api_webserver2</td>\n",
       "      <td>...</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DELETE</td>\n",
       "      <td>/api/v1/appservice/users/23026724</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-06 12:49:08</td>\n",
       "      <td>2023:06:06 12:49</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>12</td>\n",
       "      <td>interswitch-virtual-machine</td>\n",
       "      <td>102.87.162.247</td>\n",
       "      <td>36122</td>\n",
       "      <td>www-https</td>\n",
       "      <td>svaAgentLogin_backend</td>\n",
       "      <td>api_webserver5</td>\n",
       "      <td>...</td>\n",
       "      <td>781</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GET</td>\n",
       "      <td>/api/v2/quickteller/agent/accountBalance/23020622</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           xtimestamp     agg_timestamp day_of_week hour  \\\n",
       "0 2023-06-06 12:49:08  2023:06:06 12:49     Tuesday   12   \n",
       "1 2023-06-06 12:49:08  2023:06:06 12:49     Tuesday   12   \n",
       "2 2023-06-06 12:49:08  2023:06:06 12:49     Tuesday   12   \n",
       "3 2023-06-06 12:49:08  2023:06:06 12:49     Tuesday   12   \n",
       "4 2023-06-06 12:49:08  2023:06:06 12:49     Tuesday   12   \n",
       "\n",
       "                        server        client_ip  client_port   frontend  \\\n",
       "0  interswitch-virtual-machine    41.75.170.169        13805  www-https   \n",
       "1  interswitch-virtual-machine     41.75.170.75        15702  www-https   \n",
       "2  interswitch-virtual-machine    41.210.155.99        38338  www-https   \n",
       "3  interswitch-virtual-machine  197.221.137.205        34708  www-https   \n",
       "4  interswitch-virtual-machine   102.87.162.247        36122  www-https   \n",
       "\n",
       "                    backend           backendservername  ... total_time  \\\n",
       "0     svaAgentLogin_backend              api_webserver5  ...      19735   \n",
       "1  interswitchstore_backend  interswitchstore_webserver  ...        194   \n",
       "2  interswitchstore_backend  interswitchstore_webserver  ...        228   \n",
       "3               api_backend              api_webserver2  ...         72   \n",
       "4     svaAgentLogin_backend              api_webserver5  ...        781   \n",
       "\n",
       "  act_sess  fe_sess be_sess srv_sess retries  http_method  \\\n",
       "0        0        0       0        0       0          GET   \n",
       "1        0        0       0        0       0         POST   \n",
       "2        0        0       0        0       0         POST   \n",
       "3        0        0       0        0       0       DELETE   \n",
       "4        0        0       0        0       0          GET   \n",
       "\n",
       "                                            endpoint  http_version  \\\n",
       "0  /api/v2/quickteller/agent/accountBalance/23027333      HTTP/1.1   \n",
       "1         /interswitchstore/device/geolocationstatus      HTTP/1.1   \n",
       "2         /interswitchstore/device/geolocationstatus      HTTP/1.1   \n",
       "3                  /api/v1/appservice/users/23026724      HTTP/1.1   \n",
       "4  /api/v2/quickteller/agent/accountBalance/23020622      HTTP/1.1   \n",
       "\n",
       "   count_value  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(df.drop('http_status', axis=1).values)\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "01283257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtimestamp           0\n",
      "agg_timestamp        0\n",
      "day_of_week          0\n",
      "hour                 0\n",
      "server               0\n",
      "client_ip            0\n",
      "client_port          0\n",
      "frontend             0\n",
      "backend              0\n",
      "backendservername    0\n",
      "timing               0\n",
      "http_status          0\n",
      "bytes_read           0\n",
      "request_cookie       0\n",
      "response_cookie      0\n",
      "termination_state    0\n",
      "act_conn             0\n",
      "fe_conn              0\n",
      "be_conn              0\n",
      "srv_conn             0\n",
      "act_time             0\n",
      "fe_time              0\n",
      "be_time              0\n",
      "srv_time             0\n",
      "total_time           0\n",
      "act_sess             0\n",
      "fe_sess              0\n",
      "be_sess              0\n",
      "srv_sess             0\n",
      "retries              0\n",
      "http_method          0\n",
      "endpoint             0\n",
      "http_version         0\n",
      "count_value          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a1c45",
   "metadata": {},
   "source": [
    "## Perform analysis on the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c5e703df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Counts:\n",
      "/api/v1A/svapayments/validateCustomer                   333\n",
      "/interswitchstore/device/trackupdate                    195\n",
      "/interswitchstore/device/geolocationstatus              179\n",
      "/api/v2/quickteller/agent/agentLogin                    172\n",
      "/api/v1A/svapayments/sendAdviceRequest                  161\n",
      "                                                       ... \n",
      "/api/v1A/svapayments/transactions/EVERSEND1682754639      1\n",
      "/api/v2/quickteller/agent/appNotifications/3Is37494       1\n",
      "/api/v1A/svapayments/transactions/01272854120199          1\n",
      "/api/v2/quickteller/agent/accountBalance/3IS14365         1\n",
      "/api/v2/quickteller/agent/accountBalance/23029472         1\n",
      "Name: endpoint, Length: 892, dtype: int64\n",
      "\n",
      "Response Counts:\n",
      "uptime      2943\n",
      "downtime     341\n",
      "other         52\n",
      "Name: http_status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Request Analysis\n",
    "request_counts = df['endpoint'].value_counts()\n",
    "print(\"Request Counts:\")\n",
    "print(request_counts)\n",
    "\n",
    "\n",
    "# Response Analysis\n",
    "response_counts = df['http_status'].value_counts()\n",
    "print(\"\\nResponse Counts:\")\n",
    "print(response_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c786fd82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timing Summary:\n",
      "            act_time      fe_time      be_time       srv_time     total_time\n",
      "count    3336.000000  3336.000000  3336.000000    3336.000000    3336.000000\n",
      "mean    10362.869604    -0.005396     0.554257    9036.652878   19496.448741\n",
      "std     26650.251019     0.073268     1.138423   18906.966389   34318.256343\n",
      "min        -1.000000    -1.000000    -1.000000      -1.000000      35.000000\n",
      "25%       161.000000     0.000000     0.000000     195.500000     609.750000\n",
      "50%       540.500000     0.000000     0.000000     693.000000    4019.500000\n",
      "75%      5868.000000     0.000000     1.000000    7714.000000   21420.250000\n",
      "max    231518.000000     0.000000    16.000000  155133.000000  298442.000000\n"
     ]
    }
   ],
   "source": [
    "# Timing Analysis\n",
    "timing_columns = ['act_time', 'fe_time', 'be_time', 'srv_time', 'total_time']\n",
    "df[timing_columns] = df['timing'].str.split('/', expand=True).astype(int)\n",
    "timing_summary = df[timing_columns].describe()\n",
    "print(\"\\nTiming Summary:\")\n",
    "print(timing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ba8b1fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connection Summary:\n",
      "          act_conn      fe_conn      be_conn     srv_conn\n",
      "count  3336.000000  3336.000000  3336.000000  3336.000000\n",
      "mean   3994.172962  3994.172962   104.748501    55.988609\n",
      "std      53.038910    53.038910    99.072632    61.903217\n",
      "min    3924.000000  3924.000000     0.000000     0.000000\n",
      "25%    3948.000000  3948.000000    23.000000    13.000000\n",
      "50%    3974.000000  3974.000000    72.000000    39.000000\n",
      "75%    4051.000000  4051.000000   175.250000    66.000000\n",
      "max    4096.000000  4096.000000   359.000000   291.000000\n"
     ]
    }
   ],
   "source": [
    "# Connection Analysis\n",
    "connection_columns = ['act_conn', 'fe_conn', 'be_conn', 'srv_conn']\n",
    "connection_summary = df[connection_columns].describe()\n",
    "print(\"\\nConnection Summary:\")\n",
    "print(connection_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "98af6ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bytes Summary:\n",
      "count      3336.000000\n",
      "mean       3202.988309\n",
      "std       35530.296954\n",
      "min         169.000000\n",
      "25%         441.000000\n",
      "50%         517.000000\n",
      "75%         662.000000\n",
      "max      970778.000000\n",
      "Name: bytes_read, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Bytes Analysis\n",
    "bytes_summary = df['bytes_read'].describe()\n",
    "print(\"\\nBytes Summary:\")\n",
    "print(bytes_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "256263ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Analysis\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "#count_per_10min = df.resample('10T').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "90149496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# yearly_traffic.plot(kind='bar', color='blue')\n",
    "# plt.title('Yearly Traffic')\n",
    "# plt.xlabel('Year')\n",
    "# plt.ylabel('Bytes Read')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd456e",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "1. Date and Time Features: Extract additional information from the timestamp field, such as hour of the day, day of the week, month, etc. These features can help capture any temporal patterns or trends in the data.\n",
    "\n",
    "2. Categorical Encoding: Convert categorical variables like server, frontend, backend, etc., into numeric representations using one-hot encoding or label encoding. This allows machine learning models to work with categorical data effectively.\n",
    "\n",
    "3. Session Duration: Calculate the duration of each session by subtracting the start and end timestamps. This can provide insights into session lengths and help identify sessions with unusually long or short durations.\n",
    "\n",
    "4. Request Analysis: Extract features from the request_line, such as the HTTP method (GET, POST, etc.), endpoint, or API version. These features can help analyze request patterns and identify popular endpoints or API versions.\n",
    "\n",
    "5. Response Analysis: Create binary features based on the HTTP status code, such as whether the response was successful (200-299) or had an error (400-599). This can help identify patterns in successful and failed responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864d7f48",
   "metadata": {},
   "source": [
    "## Model Selection: \n",
    "Explore various deep learning models that are suitable for time-series prediction tasks. Some popular models include Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRU), Convolutional Neural Networks (CNN), or Transformer-based architectures. These models can capture temporal dependencies and patterns in the log data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0532cc",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "16b2a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Preprocess the data and prepare X and y\n",
    "# df_filtered = df[df['http_status'] != 'other']\n",
    "# X = df_filtered.drop(['http_status'], axis=1)\n",
    "# y = df_filtered['http_status']\n",
    "\n",
    "# # Encode the categorical target variable\n",
    "# encoder = OneHotEncoder(sparse=False)\n",
    "# y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# # Split the data into training and testing datasets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Build the TensorFlow model\n",
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Dense(64, activation='relu', input_shape=(2,1)),\n",
    "#     tf.keras.layers.Dense(64, activation='relu'),\n",
    "#     tf.keras.layers.Dense(2, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss, accuracy = model.evaluate(X_test, y_test)\n",
    "# print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b43d56e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['request_line'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[155], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Preprocess the data and prepare X and y\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df_filtered \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_status\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf_filtered\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbytes_read\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrequest_cookie\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresponse_cookie\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtermination_state\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mact_conn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrequest_line\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m df_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_status\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Encode the categorical variables in X\u001b[39;00m\n",
      "File \u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\pandas\\core\\frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5796\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5793\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5794\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5798\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5800\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5859\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5858\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 5859\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['request_line'] not in index\""
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Preprocess the data and prepare X and y\n",
    "df_filtered = df[df['http_status'] != 'other']\n",
    "X = df_filtered[['bytes_read', 'request_cookie', 'response_cookie', 'termination_state', 'act_conn', 'request_line']]\n",
    "y = df_filtered['http_status']\n",
    "\n",
    "# Encode the categorical variables in X\n",
    "label_encoder = LabelEncoder()\n",
    "X['request_cookie'] = label_encoder.fit_transform(X['request_cookie'])\n",
    "X['response_cookie'] = label_encoder.fit_transform(X['response_cookie'])\n",
    "X['termination_state'] = label_encoder.fit_transform(X['termination_state'])\n",
    "X['request_line'] = label_encoder.fit_transform(X['request_line'])\n",
    "\n",
    "# Scale the numerical features in X\n",
    "scaler = StandardScaler()\n",
    "X[['bytes_read', 'act_conn']] = scaler.fit_transform(X[['bytes_read', 'act_conn']])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the TensorFlow model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(6,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d894836",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc289c2",
   "metadata": {},
   "source": [
    "## Training and Evaluation:\n",
    "Split your dataset into training, validation, and testing sets. Train your deep learning model on the training data and fine-tune hyperparameters using the validation set. Evaluate the model's performance using appropriate metrics like accuracy, precision, recall, F1-score, or area under the ROC curve (AUC-ROC)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb7c28",
   "metadata": {},
   "source": [
    "## Model Interpretability:\n",
    "Consider using techniques that enhance model interpretability, especially in critical systems like predicting downtime. Methods like attention mechanisms, feature importance analysis, or SHAP (SHapley Additive exPlanations) values can help you understand the factors contributing to downtime predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3498a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
